### AbLMs Pre-training Overview

This repo uses a unified script, `ModelTrainer.py`, to pre-train all AbLMs.

Training is configured via five `config.yaml` files, each for a different model size: 8M, 35M, 150M, 350M, and 650M.

Each config supports three dataset sizes: Quarter, Half, and Full.
