vocab_size: 33  
pad_token_id: 1  
mask_token_id: 32  
num_attention_heads: 20  
num_hidden_layers: 6  
hidden_size: 320  
intermediate_size: 1280  
max_position_embeddings: 322   
position_embedding_type: "rotary"  
  
# Tokenizer  
tokenizer_path: "facebook/esm2_t6_8M_UR50D"   
padding: "max_length"  
max_length: 320  
truncation: True  
return_special_tokens_mask: False  
separator_token: "<cls><cls>"  
  
# --- Datasets ---
# The 'validation_file' and 'file_type' are common for all training configurations.
#Update train_file with your selected dataset size/path (full data- half data _ quarter data) in : train_file.

validation_file: "./data/eval/validation_dataset.csv"
file_type: "csv" # Assuming CSV for all train/validation files

# Option 1: Full Training Dataset (Currently Active)
train_file: "./data/full/training_full.csv"

# Option 2: Half Training Dataset
# To use, comment out the 'train_file' for the Full Dataset above and uncomment the line below.
# train_file: "./data/half/training_half.csv"


# Option 3: Quarter Training Dataset
# train_file: "./data/quarter/training_quarter.csv"


# Collator  
mlm: True  
mlm_probability: 0.15  
  
# --- Training Arguments ---

# Modify 'run_name' to reflect the dataset size (e.g., "m_8M_half_batch_128") if not training on the 'full' dataset.
run_name: "m_8M_full_batch_128"
fp16: True  
seed: 42  
batch_size: 32  
gradient_accumulation_steps: 1  
logging_steps: 100  
evaluation_strategy: "steps"  
eval_steps: 5000  
save_steps: 5000  
  
warmup_steps: 30000  
max_steps: 500000  
  
peak_learning_rate: 0.0001  
weight_decay: 0.01  
adam_epsilon: 0.000001  # Consistent scientific notation  
adam_beta1: 0.9  
adam_beta2: 0.98  
  
overwrite_output_dir: False  
output_dir: "./01all_models/deepspeed/esm/all_checkpoints_4good/{run_name}"
report_to: "wandb"  
wandb_project: "AbLM_TRAIN"  # project name  
logging_first_step: True  
logging_dir: "./wandb/{run_name}"  
