vocab_size: 33
pad_token_id: 1
mask_token_id: 32
num_attention_heads: 20
num_hidden_layers: 33
hidden_size: 1280
intermediate_size: 5120
max_position_embeddings: 322
position_embedding_type: "rotary"

# Tokenizer  
tokenizer_path: "facebook/esm2_t33_650M_UR50D"
padding: "max_length"
max_length: 320
truncation: True
return_special_tokens_mask: False
separator_token: "<cls><cls>"

# --- Datasets ---
# The 'validation_file' and 'file_type' are common for all training configurations.
# Select the desired training dataset size by uncommenting one of the 'train_file' options below.

validation_file: "./validation/validation_dataset.csv"
file_type: "csv" # Assuming CSV for all train/validation files

# Option 1: Full Training Dataset (Currently Active)
train_file: "./training/training_full.csv"

# Option 2: Half Training Dataset
# To use, comment out the 'train_file' for the Full Dataset above and uncomment the line below.
# train_file: "./training/training_half.csv"

# Option 3: Quarter Training Dataset
# To use, comment out the 'train_file' for the Full Dataset above (or Half, if active) and uncomment the line below.
# train_file: "./training/training_quarter.csv"


# Collator  
mlm: True
mlm_probability: 0.15

# --- Training Arguments ---

# Modify 'run_name' to reflect the dataset size (e.g., "esm_650M_half_batch_128") if not training on the 'full' dataset.
run_name: "esm_650M_full_batch_128"
fp16: True
seed: 42
batch_size: 16
gradient_accumulation_steps: 1
logging_steps: 100
evaluation_strategy: "steps"
eval_steps: 5000
save_steps: 5000

warmup_steps: 30000
max_steps: 500000

peak_learning_rate: 0.0001
weight_decay: 0.01
adam_epsilon: 0.000001
adam_beta1: 0.9
adam_beta2: 0.98

overwrite_output_dir: False
output_dir: "/home/jovyan/shared/mahdi/1_projects/model_optimization/01all_esm_models/deepspeed/esm/all_checkpoints_4good/{run_name}"
report_to: "wandb"
wandb_project: "ESM_TRAIN"  # Update this with your own project!  
logging_first_step: True
logging_dir: "./wandb/{run_name}"               
