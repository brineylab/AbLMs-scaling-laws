{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b46a8d13-1f40-45a4-8ade-70e85aa81744",
   "metadata": {},
   "source": [
    "## Evaluating AbLMs on test tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48575f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    EsmTokenizer,\n",
    "    EsmForMaskedLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac05e67-b197-4330-919c-8a96a5a83cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare your dataset\n",
    "dataset = load_dataset(\n",
    "    'csv',\n",
    "    data_files={'test': './data/test/test_dataset.csv'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b175cc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = EsmTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "\n",
    "model_path = './01all_esm_models/deepspeed/esm/all_checkpoints_4good/m_150M_full_batch_128_2025-02-11/checkpoint-500000'\n",
    "model = EsmForMaskedLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c786976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for tokenization\n",
    "MAX_LEN = 320  # e.g. train_config[\"max_length\"]\n",
    "SEPARATOR = \"<cls><cls>\"  # e.g. train_config[\"separator_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5521bdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "def preprocess_function(example):\n",
    "    # Combine the heavy and light chain sequences\n",
    "    sequence = example['sequence_aa_heavy'] + SEPARATOR + example['sequence_aa_light']\n",
    "    # Tokenize with the same settings used during training\n",
    "    tokenized = tokenizer(\n",
    "        sequence,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "    # Add special tokens mask if required (as done in training)\n",
    "    tokenized['special_tokens_mask'] = tokenizer.get_special_tokens_mask(tokenized['input_ids'], already_has_special_tokens=True)\n",
    "    return tokenized\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=False)\n",
    "eval_dataset = tokenized_datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854f2792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15  # Use the same probability as in training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eff5fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup evaluation arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_dir='./logs',\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"no\",\n",
    "    report_to=\"none\",  # Explicitly disable W&B logging\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22b6774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "# Print the eval_loss (average cross‑entropy per token)\n",
    "loss = eval_results['eval_loss']\n",
    "print(f\"Cross‑Entropy Loss (eval_loss): {loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
