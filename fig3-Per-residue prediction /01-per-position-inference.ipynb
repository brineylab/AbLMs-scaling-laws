{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9595a985-0301-468a-992d-32a254178aac",
   "metadata": {},
   "source": [
    "# Per-position Inference for models trained on full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12120e4f-c1e4-4626-8845-d660f5c23db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as mpatches  \n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas(leave = False)\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    EsmTokenizer,\n",
    "    EsmForMaskedLM,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "from itertools import chain\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a579db-bd1c-4496-a030-6ab8ab7af06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34adb432-d9a9-482f-943b-a21091e24e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict_full= {\n",
    "    \"m_8M_F\": \"./01all_esm_models/deepspeed/esm/all_checkpoints_4good/m_8M_full_batch_128_2025-02-10/checkpoint-500000\",\n",
    "    \"m_35M_F\": \"./01all_esm_models/deepspeed/esm/all_checkpoints_4good/m_35M_full_batch_128_2025-02-10/checkpoint-500000\",\n",
    "    \"m_150M_F\": \"./01all_esm_models/deepspeed/esm/all_checkpoints_4good/m_150M_full_batch_128_2025-02-11/checkpoint-500000\",\n",
    "    \"m_350M_F\": \"./01all_esm_models/deepspeed/esm/all_checkpoints_4good/m_350M_full_batch_128_2025-01-29/checkpoint-500000\",\n",
    "    \"m_650M_F\": \"./01all_esm_models/deepspeed/esm/all_checkpoints_4good/m_650M_full_batch_128_2025-01-29/checkpoint-395000\",\n",
    "    \n",
    "}\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = EsmTokenizer.from_pretrained(\"facebook/esm2_t12_35M_UR50D\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c2f0f7",
   "metadata": {},
   "source": [
    "## metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4527d3db-ea1a-485e-85af-8ad4b0432df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_and_group_stats(model, tokenizer, seq, cdr):\n",
    "    losses = []\n",
    "    predictions = \"\"\n",
    "    scores = []\n",
    "    perplexities = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        sep = \"<cls><cls>\"\n",
    "        sep_idx = seq.find(sep)\n",
    "        heavy = seq[:sep_idx]\n",
    "        light = seq[sep_idx + len(sep):]\n",
    "        cdr_mask = cdr[:sep_idx] + cdr[sep_idx + 2:]\n",
    "\n",
    "        unmasked = tokenizer(seq, return_tensors = \"pt\").to(device)[\"input_ids\"]\n",
    "        ranges = [range(sep_idx), range(sep_idx + len(sep), len(seq))]\n",
    "        total_len = sum(len(i) for i in ranges)\n",
    "\n",
    "        # model iteratively predicts each residue (skipping over separator tokens)\n",
    "        for i in chain(*ranges):\n",
    "        # for i in tqdm(chain(*ranges), total=total_len, leave=False):\n",
    "            masked = seq[:i] + \"<mask>\" + seq[i+1:]\n",
    "            tokenized = tokenizer(masked, return_tensors=\"pt\").to(device)\n",
    "            mask_pos = (tokenized.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "            labels = torch.where(tokenized.input_ids == tokenizer.mask_token_id, unmasked, -100)\n",
    "            output = model(**tokenized, labels = labels)\n",
    "            logits = output.logits\n",
    "\n",
    "            # predicted aa\n",
    "            pred_token = logits[0, mask_pos].argmax(axis=-1)\n",
    "            predictions+=tokenizer.decode(pred_token)\n",
    "\n",
    "            # prediction confidence\n",
    "            prob = logits[0, mask_pos].softmax(dim=-1).topk(1)[0].item()\n",
    "            scores.append(prob)\n",
    "\n",
    "            # loss\n",
    "            loss = output.loss.item()\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # perplexity\n",
    "            ce_loss = F.cross_entropy(logits.view(-1, tokenizer.vocab_size), labels.view(-1)) # i think this is the same as output.loss.item()\n",
    "            perplexities.append(float(torch.exp(ce_loss)))\n",
    "\n",
    "        # group stats by region\n",
    "        # find indices splitting regions (fwrs and cdrs in heavy and light chains)\n",
    "        cdr_idxs = [0] + [i for i in range(len(cdr_mask)) if cdr_mask[i] != cdr_mask[i-1]] + [len(cdr_mask)]\n",
    "        cdr_idxs.insert(7, sep_idx)\n",
    "        \n",
    "        # accuracy\n",
    "        predictions_by_region = [predictions[cdr_idxs[n]:cdr_idxs[n+1]] for n in range(len(cdr_idxs)-1)]\n",
    "        seq_by_region = [seq.replace(sep, \"\")[cdr_idxs[n]:cdr_idxs[n+1]] for n in range(len(cdr_idxs)-1)]\n",
    "        region_mean_acc = [sum(true[i] == predict[i] for i in range(len(true)))/len(true) for true, predict in zip(seq_by_region, predictions_by_region)]\n",
    "\n",
    "        # prediction confidence\n",
    "        region_mean_scores = [np.mean(scores[cdr_idxs[n]:cdr_idxs[n+1]]) for n in range(len(cdr_idxs)-1)]\n",
    "\n",
    "        # loss (median)\n",
    "        region_median_loss = [np.median(losses[cdr_idxs[n]:cdr_idxs[n+1]]) for n in range(len(cdr_idxs)-1)]\n",
    "        \n",
    "        # perplexity\n",
    "        region_mean_perplexity = [np.mean(perplexities[cdr_idxs[n]:cdr_idxs[n+1]]) for n in range(len(cdr_idxs)-1)]\n",
    "        \n",
    "        return {\n",
    "            \"sequence\": seq.replace(sep, \"\"),\n",
    "            \"heavy\": heavy,\n",
    "            \"light\": light,\n",
    "            \"cdr_indices\": cdr_idxs,\n",
    "            \"prediction\": predictions,\n",
    "            \"accuracy_by_region\": region_mean_acc,\n",
    "            \"score_by_region\": region_mean_scores,\n",
    "            \"loss_by_region\": region_median_loss,\n",
    "            \"perplexity_by_region\": region_mean_perplexity,\n",
    "            \"score\": scores,\n",
    "            \"loss\": losses,\n",
    "            \"perplexity\": perplexities\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599911da-b57b-453f-8344-f761f0385377",
   "metadata": {},
   "source": [
    "## Load test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "951ed86d-6581-4b91-9094-3add69db4602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to seed the random state for the splits\n",
    "seed = 42\n",
    "\n",
    "# test data separated in unmutated (germline) and mutated \n",
    "germline_test_df = pd.read_csv(\"./04Per_residue_inference/GERMLINE_annotated_with_cdr_mask_last_version.csv\")\n",
    "mutated_test_df = pd.read_csv(\"./04Per_residue_inference/MUTATED_annotated_with_cdr_mask_last_version.csv\")\n",
    "\n",
    "# format 2000 sample sequences for model inference\n",
    "germline_test_df = germline_test_df.sample(n = 2000, random_state = seed)\n",
    "mutated_test_df = mutated_test_df.sample(n = 2000, random_state = seed)\n",
    "\n",
    "data_dict = {\n",
    "    \"germline\": germline_test_df,\n",
    "    \"mutated\": mutated_test_df,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2649deaf-9b93-4ea7-9a2f-3c1f604fbf46",
   "metadata": {},
   "source": [
    "### Inference on 2000 sequences across models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b269f8-fe3a-442c-b7ae-0867fec40608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 8M_F germline\n",
    "# Specify the model and data keys  \n",
    "name = 'm_8M_F'  # Same model as before  \n",
    "seq_type = 'germline'  # Replace with the actual key from data_dict  \n",
    "  \n",
    "# Retrieve the model path and data  \n",
    "model_path = model_dict_full[name]  \n",
    "data = data_dict[seq_type]  \n",
    "  \n",
    "# Load the model  \n",
    "model = EsmForMaskedLM.from_pretrained(model_path).to(device)  \n",
    "  \n",
    "# Perform inference  \n",
    "inference_data = []  \n",
    "sequences = list(data.iterrows())  \n",
    "  \n",
    "for _id, row in tqdm(sequences):  \n",
    "    d = infer_and_group_stats(  \n",
    "        model,  \n",
    "        tokenizer,  \n",
    "        row['text'],  \n",
    "        row['cdr_mask']  \n",
    "    )  \n",
    "    inference_data.append(d)  \n",
    "  \n",
    "# Create a DataFrame from the inference data  \n",
    "inference_df = pd.DataFrame(inference_data)  \n",
    "  \n",
    "# Save the results  \n",
    "inference_df.to_json(f\"./results_full/{name}_{seq_type}_{len(sequences)}.json\")  \n",
    "  \n",
    "print(f\"Inference completed for {name} on {seq_type}. Results saved.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee9e656-9f88-408d-8107-52b5001fe4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 8M_F mutated\n",
    "# Specify the model and data keys  \n",
    "name = 'm_8M_F'  # Same model as before  \n",
    "seq_type = 'mutated'  # Replace with the actual key from data_dict  \n",
    "  \n",
    "# Retrieve the model path and data  \n",
    "model_path = model_dict_full[name]  \n",
    "data = data_dict[seq_type]  \n",
    "  \n",
    "# Load the model  \n",
    "model = EsmForMaskedLM.from_pretrained(model_path).to(device)  \n",
    "  \n",
    "# Perform inference  \n",
    "inference_data = []  \n",
    "sequences = list(data.iterrows())  \n",
    "  \n",
    "for _id, row in tqdm(sequences):  \n",
    "    d = infer_and_group_stats(  \n",
    "        model,  \n",
    "        tokenizer,  \n",
    "        row['text'],  \n",
    "        row['cdr_mask']  \n",
    "    )  \n",
    "    inference_data.append(d)  \n",
    "  \n",
    "# Create a DataFrame from the inference data  \n",
    "inference_df = pd.DataFrame(inference_data)  \n",
    "  \n",
    "# Save the results  \n",
    "inference_df.to_json(f\"./results_full/{name}_{seq_type}_{len(sequences)}.json\")  \n",
    "  \n",
    "print(f\"Inference completed for {name} on {seq_type}. Results saved.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca08d3f-4275-4455-8034-19b9a157c6be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model 35M_F germline\n",
    "name = 'm_35M_F'  # Same model as before  \n",
    "seq_type = 'germline'  # Replace with the actual key from data_dict  \n",
    "  \n",
    "# Retrieve the model path and data  \n",
    "model_path = model_dict_full[name]  \n",
    "data = data_dict[seq_type]  \n",
    "  \n",
    "# Load the model  \n",
    "model = EsmForMaskedLM.from_pretrained(model_path).to(device)  \n",
    "  \n",
    "# Perform inference  \n",
    "inference_data = []  \n",
    "sequences = list(data.iterrows())  \n",
    "  \n",
    "for _id, row in tqdm(sequences):  \n",
    "    d = infer_and_group_stats(  \n",
    "        model,  \n",
    "        tokenizer,  \n",
    "        row['text'],  \n",
    "        row['cdr_mask']  \n",
    "    )  \n",
    "    inference_data.append(d)  \n",
    "  \n",
    "# Create a DataFrame from the inference data  \n",
    "inference_df = pd.DataFrame(inference_data)  \n",
    "  \n",
    "# Save the results  \n",
    "inference_df.to_json(f\"./results_full/{name}_{seq_type}_{len(sequences)}.json\")  \n",
    "  \n",
    "print(f\"Inference completed for {name} on {seq_type}. Results saved.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f05881-af28-4b04-bf9c-a217b64b8ce9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model 35M_F mutated\n",
    "\n",
    "name = 'm_35M_F'  # Same model as before  \n",
    "seq_type = 'mutated'  # Replace with the actual key from data_dict  \n",
    "  \n",
    "# Retrieve the model path and data  \n",
    "model_path = model_dict_full[name]  \n",
    "data = data_dict[seq_type]  \n",
    "  \n",
    "# Load the model  \n",
    "model = EsmForMaskedLM.from_pretrained(model_path).to(device)  \n",
    "  \n",
    "# Perform inference  \n",
    "inference_data = []  \n",
    "sequences = list(data.iterrows())  \n",
    "  \n",
    "for _id, row in tqdm(sequences):  \n",
    "    d = infer_and_group_stats(  \n",
    "        model,  \n",
    "        tokenizer,  \n",
    "        row['text'],  \n",
    "        row['cdr_mask']  \n",
    "    )  \n",
    "    inference_data.append(d)  \n",
    "  \n",
    "# Create a DataFrame from the inference data  \n",
    "inference_df = pd.DataFrame(inference_data)  \n",
    "  \n",
    "# Save the results  \n",
    "inference_df.to_json(f\"./results_full/{name}_{seq_type}_{len(sequences)}.json\")  \n",
    "  \n",
    "print(f\"Inference completed for {name} on {seq_type}. Results saved.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7832795",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "name = 'm_150M_F'  # Same model as before  \n",
    "seq_type = 'germline'  # Replace with the actual key from data_dict  \n",
    "  \n",
    "# Retrieve the model path and data  \n",
    "model_path = model_dict_full[name]  \n",
    "data = data_dict[seq_type]  \n",
    "  \n",
    "# Load the model  \n",
    "model = EsmForMaskedLM.from_pretrained(model_path).to(device)  \n",
    "  \n",
    "# Perform inference  \n",
    "inference_data = []  \n",
    "sequences = list(data.iterrows())  \n",
    "  \n",
    "for _id, row in tqdm(sequences):  \n",
    "    d = infer_and_group_stats(  \n",
    "        model,  \n",
    "        tokenizer,  \n",
    "        row['text'],  \n",
    "        row['cdr_mask']  \n",
    "    )  \n",
    "    inference_data.append(d)  \n",
    "  \n",
    "# Create a DataFrame from the inference data  \n",
    "inference_df = pd.DataFrame(inference_data)  \n",
    "  \n",
    "# Save the results  \n",
    "inference_df.to_json(f\"./results_full/{name}_{seq_type}_{len(sequences)}.json\")  \n",
    "  \n",
    "print(f\"Inference completed for {name} on {seq_type}. Results saved.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17930b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "name = 'm_150M_F'  # Same model as before  \n",
    "seq_type = 'mutated'  # Replace with the actual key from data_dict  \n",
    "  \n",
    "# Retrieve the model path and data  \n",
    "model_path = model_dict_full[name]  \n",
    "data = data_dict[seq_type]  \n",
    "  \n",
    "# Load the model  \n",
    "model = EsmForMaskedLM.from_pretrained(model_path).to(device)  \n",
    "  \n",
    "# Perform inference  \n",
    "inference_data = []  \n",
    "sequences = list(data.iterrows())  \n",
    "  \n",
    "for _id, row in tqdm(sequences):  \n",
    "    d = infer_and_group_stats(  \n",
    "        model,  \n",
    "        tokenizer,  \n",
    "        row['text'],  \n",
    "        row['cdr_mask']  \n",
    "    )  \n",
    "    inference_data.append(d)  \n",
    "  \n",
    "# Create a DataFrame from the inference data  \n",
    "inference_df = pd.DataFrame(inference_data)  \n",
    "  \n",
    "# Save the results  \n",
    "inference_df.to_json(f\"./results_full/{name}_{seq_type}_{len(sequences)}.json\")  \n",
    "  \n",
    "print(f\"Inference completed for {name} on {seq_type}. Results saved.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e611364-28ec-4a61-9489-68b880f06fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "name = 'm_350M_F'  # Same model as before  \n",
    "seq_type = 'germline'  # Replace with the actual key from data_dict  \n",
    "  \n",
    "# Retrieve the model path and data  \n",
    "model_path = model_dict_full[name]  \n",
    "data = data_dict[seq_type]  \n",
    "  \n",
    "# Load the model  \n",
    "model = EsmForMaskedLM.from_pretrained(model_path).to(device)  \n",
    "  \n",
    "# Perform inference  \n",
    "inference_data = []  \n",
    "sequences = list(data.iterrows())  \n",
    "  \n",
    "for _id, row in tqdm(sequences):  \n",
    "    d = infer_and_group_stats(  \n",
    "        model,  \n",
    "        tokenizer,  \n",
    "        row['text'],  \n",
    "        row['cdr_mask']  \n",
    "    )  \n",
    "    inference_data.append(d)  \n",
    "  \n",
    "# Create a DataFrame from the inference data  \n",
    "inference_df = pd.DataFrame(inference_data)  \n",
    "  \n",
    "# Save the results  \n",
    "inference_df.to_json(f\"./results_full/{name}_{seq_type}_{len(sequences)}.json\")  \n",
    "  \n",
    "print(f\"Inference completed for {name} on {seq_type}. Results saved.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5ece1d-fb81-4917-ba0b-5abec5071d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "name = 'm_350M_F'  # Same model as before  \n",
    "seq_type = 'mutated'  # Replace with the actual key from data_dict  \n",
    "  \n",
    "# Retrieve the model path and data  \n",
    "model_path = model_dict_full[name]  \n",
    "data = data_dict[seq_type]  \n",
    "  \n",
    "# Load the model  \n",
    "model = EsmForMaskedLM.from_pretrained(model_path).to(device)  \n",
    "  \n",
    "# Perform inference  \n",
    "inference_data = []  \n",
    "sequences = list(data.iterrows())  \n",
    "  \n",
    "for _id, row in tqdm(sequences):  \n",
    "    d = infer_and_group_stats(  \n",
    "        model,  \n",
    "        tokenizer,  \n",
    "        row['text'],  \n",
    "        row['cdr_mask']  \n",
    "    )  \n",
    "    inference_data.append(d)  \n",
    "  \n",
    "# Create a DataFrame from the inference data  \n",
    "inference_df = pd.DataFrame(inference_data)  \n",
    "  \n",
    "# Save the results  \n",
    "inference_df.to_json(f\"./results_full/{name}_{seq_type}_{len(sequences)}.json\")  \n",
    "  \n",
    "print(f\"Inference completed for {name} on {seq_type}. Results saved.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6ae7cd-444b-4785-8f48-c94490472e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "name = 'm_650M_F'  # Same model as before  \n",
    "seq_type = 'germline'  # Replace with the actual key from data_dict  \n",
    "  \n",
    "# Retrieve the model path and data  \n",
    "model_path = model_dict_full[name]  \n",
    "data = data_dict[seq_type]  \n",
    "  \n",
    "# Load the model  \n",
    "model = EsmForMaskedLM.from_pretrained(model_path).to(device)  \n",
    "  \n",
    "# Perform inference  \n",
    "inference_data = []  \n",
    "sequences = list(data.iterrows())  \n",
    "  \n",
    "for _id, row in tqdm(sequences):  \n",
    "    d = infer_and_group_stats(  \n",
    "        model,  \n",
    "        tokenizer,  \n",
    "        row['text'],  \n",
    "        row['cdr_mask']  \n",
    "    )  \n",
    "    inference_data.append(d)  \n",
    "  \n",
    "# Create a DataFrame from the inference data  \n",
    "inference_df = pd.DataFrame(inference_data)  \n",
    "  \n",
    "# Save the results  \n",
    "inference_df.to_json(f\"./results_full/{name}_{seq_type}_{len(sequences)}.json\")  \n",
    "  \n",
    "print(f\"Inference completed for {name} on {seq_type}. Results saved.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4afea4-812b-4751-abf8-d0c50efedcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "name = 'm_650M_F'  # Same model as before  \n",
    "seq_type = 'mutated'  # Replace with the actual key from data_dict  \n",
    "  \n",
    "# Retrieve the model path and data  \n",
    "model_path = model_dict_full[name]  \n",
    "data = data_dict[seq_type]  \n",
    "  \n",
    "# Load the model  \n",
    "model = EsmForMaskedLM.from_pretrained(model_path).to(device)  \n",
    "  \n",
    "# Perform inference  \n",
    "inference_data = []  \n",
    "sequences = list(data.iterrows())  \n",
    "  \n",
    "for _id, row in tqdm(sequences):  \n",
    "    d = infer_and_group_stats(  \n",
    "        model,  \n",
    "        tokenizer,  \n",
    "        row['text'],  \n",
    "        row['cdr_mask']  \n",
    "    )  \n",
    "    inference_data.append(d)  \n",
    "  \n",
    "# Create a DataFrame from the inference data  \n",
    "inference_df = pd.DataFrame(inference_data)  \n",
    "  \n",
    "# Save the results  \n",
    "inference_df.to_json(f\"./results_full/{name}_{seq_type}_{len(sequences)}.json\")  \n",
    "  \n",
    "print(f\"Inference completed for {name} on {seq_type}. Results saved.\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
